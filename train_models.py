# train_models.py
# This script trains a separate machine learning model for each ethical mode
# using the labeled datasets generated by label_data.py.
# Each model is stored as a .pkl file for later use in the Streamlit demo.

import os                      # For file and directory handling
import pandas as pd            # For loading CSV datasets
from sklearn.model_selection import train_test_split  # For splitting data into train/test sets
from sklearn.pipeline import Pipeline                 # For building ML workflows
from sklearn.compose import ColumnTransformer         # For handling mixed numeric/categorical features
from sklearn.preprocessing import OneHotEncoder       # For encoding categorical features
from sklearn.ensemble import RandomForestClassifier   # ML model type
from sklearn.metrics import classification_report     # For evaluating performance
import joblib                  # For saving trained models to disk

# Ethical modes to train models for
MODES = ["utilitarian", "deontological", "virtue"]

# Directories for input data and output models
DATA_DIR = "labeled_data"  # CSVs from label_data.py
MODEL_DIR = "models"       # Where trained models will be saved

# List of features
FEATURES_NUM = ["left_risk", "right_risk", "speed_kph"]  # Numeric inputs
FEATURES_CAT = ["name", "child_present"]                 # Categorical inputs
FEATURES = FEATURES_NUM + FEATURES_CAT                   # Combined list

if __name__ == "__main__":
    # Creates the model directory if it does not exist
    os.makedirs(MODEL_DIR, exist_ok=True)

    # Iterates over each ethical mode
    for mode in MODES:
        # Loads the corresponding labeled dataset
        df = pd.read_csv(os.path.join(DATA_DIR, f"{mode}_labeled.csv"))

        # Splits into features (X) and target (y)
        X = df[FEATURES].copy()
        y = df["action"].astype(str)

        # Splits data into training and testing sets (80/20 split)
        Xtr, Xte, ytr, yte = train_test_split(
            X, y,
            test_size=0.2,
            random_state=7,
            stratify=y  # Ensures action classes are evenly distributed
        )

        # Defines preprocessing:
        # - Pass numeric features through unchanged
        # - Apply one-hot encoding to categorical features
        pre = ColumnTransformer(
            transformers=[
                ("num", "passthrough", FEATURES_NUM),
                ("cat", OneHotEncoder(handle_unknown="ignore"), FEATURES_CAT),
            ],
            remainder="drop",  # Drops unused columns
        )

        # Defines the classifier:
        # - Random Forest with 350 trees
        # - Balanced weights for classes
        # - Uses all CPU cores available
        clf = RandomForestClassifier(
            n_estimators=350,
            random_state=7,
            class_weight="balanced_subsample",
            n_jobs=-1
        )

        # Builds the pipeline: preprocessing → model
        pipe = Pipeline([("pre", pre), ("clf", clf)])

        # Fits the pipeline to the training data
        pipe.fit(Xtr, ytr)

        # Prints evaluation metrics on the test set
        print(f"\n=== {mode} ===")
        print(classification_report(yte, pipe.predict(Xte)))

        # Saves the trained pipeline to a .pkl file
        model_path = os.path.join(MODEL_DIR, f"{mode}.pkl")
        joblib.dump(pipe, model_path)
        print(f"✅ saved {model_path}")
